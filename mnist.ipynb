{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset = datasets.MNIST('../mnist_data/',\n",
    "                            download=True,\n",
    "                            train=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,),(0.3081,))\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datasets.MNIST('../mnist_data/',\n",
    "                            download=False,\n",
    "                            train=False,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,),(0.3081,))\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trn_loader = torch.utils.data.DataLoader(trn_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        conv1 = nn.Conv2d(1,6,5,1)  #6@24*24\n",
    "        # activation relu\n",
    "        pool1 = nn.MaxPool2d(2)\n",
    "        conv2 = nn.Conv2d(6,16,5,1)  #16@8*8\n",
    "        # activation relu\n",
    "        pool2 = nn.MaxPool2d(2)  #16@4*4\n",
    "        \n",
    "        self.conv_module = nn.Sequential(\n",
    "                    conv1,\n",
    "                    nn.ReLU(),\n",
    "                    pool1,\n",
    "                    conv2,\n",
    "                    nn.ReLU(),\n",
    "                    pool2)\n",
    "        fc1 = nn.Linear(16*4*4, 120)\n",
    "        # activation relu\n",
    "        fc2 = nn.Linear(120, 84)\n",
    "        # activation relu\n",
    "        fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        self.fc_module = nn.Sequential(\n",
    "                            fc1,\n",
    "                            nn.ReLU(),\n",
    "                            fc2,\n",
    "                            nn.ReLU(),\n",
    "                            fc3)\n",
    "        # gpu할당\n",
    "        if use_cuda:\n",
    "            self.conv_module = self.conv_module.cuda()\n",
    "            self.fc_module = self.fc_module.cuda()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.conv_module(x)\n",
    "        # make linear\n",
    "        dim = 1\n",
    "        for d in out.size()[1:]:\n",
    "            dim = dim*d\n",
    "        out = out.view(-1, dim)\n",
    "        out = self.fc_module(out)\n",
    "        return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "cnn = CNNClassifier()\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# backpropagation\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "# hypter-parameters\n",
    "num_epochs = 2\n",
    "num_batches = len(trn_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/2 | step: 100/938 | trn loss: 1.6237 | val loss: 1.5429\n",
      "epoch: 1/2 | step: 200/938 | trn loss: 1.5407 | val loss: 1.5239\n",
      "epoch: 1/2 | step: 300/938 | trn loss: 1.5240 | val loss: 1.5115\n",
      "epoch: 1/2 | step: 400/938 | trn loss: 1.5151 | val loss: 1.5078\n",
      "epoch: 1/2 | step: 500/938 | trn loss: 1.5086 | val loss: 1.5069\n",
      "epoch: 1/2 | step: 600/938 | trn loss: 1.5079 | val loss: 1.4973\n",
      "epoch: 1/2 | step: 700/938 | trn loss: 1.5008 | val loss: 1.4983\n",
      "epoch: 1/2 | step: 800/938 | trn loss: 1.5012 | val loss: 1.4984\n",
      "epoch: 1/2 | step: 900/938 | trn loss: 1.5004 | val loss: 1.4933\n",
      "epoch: 2/2 | step: 100/938 | trn loss: 1.4946 | val loss: 1.4877\n",
      "epoch: 2/2 | step: 200/938 | trn loss: 1.4883 | val loss: 1.4919\n",
      "epoch: 2/2 | step: 300/938 | trn loss: 1.4944 | val loss: 1.4865\n",
      "epoch: 2/2 | step: 400/938 | trn loss: 1.4914 | val loss: 1.4966\n",
      "epoch: 2/2 | step: 500/938 | trn loss: 1.4914 | val loss: 1.4891\n",
      "epoch: 2/2 | step: 600/938 | trn loss: 1.4901 | val loss: 1.4814\n",
      "epoch: 2/2 | step: 700/938 | trn loss: 1.4890 | val loss: 1.4874\n",
      "epoch: 2/2 | step: 800/938 | trn loss: 1.4870 | val loss: 1.4870\n",
      "epoch: 2/2 | step: 900/938 | trn loss: 1.4868 | val loss: 1.4889\n"
     ]
    }
   ],
   "source": [
    "trn_loss_list, val_loss_list = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    trn_loss = 0\n",
    "    for i, data in enumerate(trn_loader):\n",
    "        x, label = data\n",
    "        if use_cuda:\n",
    "            x, label = x.cuda(), label.cuda()\n",
    "        # grad_init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        model_output = cnn(x)\n",
    "        # calculate_loss\n",
    "        loss = criterion(model_output, label)\n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del(memory issue)\n",
    "        del loss\n",
    "        del model_output\n",
    "        \n",
    "        # 학습과정 출력\n",
    "        if (i+1) % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for j, val in enumerate(val_loader):\n",
    "                    val_x, val_label = val\n",
    "                    if use_cuda:\n",
    "                        val_x, val_label = val_x.cuda(), val_label.cuda()\n",
    "                    val_output = cnn(val_x)\n",
    "                    v_loss = criterion(val_output, val_label)\n",
    "                    val_loss += v_loss\n",
    "                    \n",
    "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n",
    "                    epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader)))\n",
    "            \n",
    "            trn_loss_list.append(trn_loss/100)\n",
    "            val_loss_list.append(val_loss/len(val_loader))\n",
    "            trn_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkh36",
   "language": "python",
   "name": "kkh36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
